# üìä Real-Time Player Performance Analytics ‚Äì Big Data

A real-time big data analytics system designed to ingest, process, and analyze player performance metrics at scale using distributed streaming technologies.

---

## üöÄ Project Overview

This project implements a **real-time big data pipeline** for analyzing live player performance data.  
It leverages **event-driven streaming**, **automated data ingestion**, and **distributed processing** to generate actionable insights in near real time.

---

## üõ†Ô∏è Technologies Used

- Apache Kafka ‚Äì Real-time event streaming
- Apache NiFi ‚Äì Data ingestion and flow orchestration
- Apache Spark ‚Äì Distributed stream processing and analytics
- PostgreSQL ‚Äì Persistent data storage
- Python ‚Äì Producer & consumer implementation
- Apache ZooKeeper ‚Äì Kafka coordination

---

## üß± System Architecture

```
Kafka Producer (Python)
        ‚Üì
     Apache Kafka
        ‚Üì
     Apache NiFi
        ‚Üì
 Apache Spark Streaming
        ‚Üì
    PostgreSQL
```

---

## üìã Prerequisites

Make sure the following are installed and running:

- Java 8+
- Python 3.8+
- Apache ZooKeeper
- Apache Kafka
- Apache NiFi
- Apache Spark
- PostgreSQL

---

## ‚öôÔ∏è Setup & Installation

### 1Ô∏è‚É£ Start ZooKeeper
```bash
bin/zookeeper-server-start.sh config/zookeeper.properties
```

---

### 2Ô∏è‚É£ Start Apache Kafka
```bash
bin/kafka-server-start.sh config/server.properties
```

Create a Kafka topic:
```bash
bin/kafka-topics.sh --create \
  --topic player-performance \
  --bootstrap-server localhost:9092 \
  --partitions 1 \
  --replication-factor 1
```

---

### 3Ô∏è‚É£ Start Apache NiFi
```bash
bin/nifi.sh start
```

Access the NiFi UI:
```
http://localhost:8080/nifi
```

Configure NiFi to consume data from Kafka and forward it to Spark.

> ‚ö†Ô∏è Do NOT hardcode credentials.  
> Use environment variables or configuration files.

---

### 4Ô∏è‚É£ Setup PostgreSQL

Create database and table:
```sql
CREATE DATABASE player_analytics;

CREATE TABLE player_metrics (
    id SERIAL PRIMARY KEY,
    player_id VARCHAR(50),
    metric_name VARCHAR(100),
    metric_value FLOAT,
    event_time TIMESTAMP
);
```

---

## ‚ñ∂Ô∏è Running the Project

### Kafka Producer
Streams player performance events to Kafka:
```bash
spark-submit kafka_producer.py
```

---

### Spark Consumer
Consumes Kafka streams, processes data, and stores results in PostgreSQL:
```bash
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.1,\
org.apache.spark:spark-streaming-kafka-0-10_2.13:4.0.1,\
org.postgresql:postgresql:42.7.3 \
  spark_consumer.py
```

---

## üìà Output

- Real-time processed player performance metrics
- Structured data stored in PostgreSQL
- Scalable and fault-tolerant streaming pipeline

---

## üîí Security Notes

- Never commit usernames or passwords to GitHub
- Use `.env` files or environment variables
- Add `.env` to `.gitignore`

---

## üìΩÔ∏è Demo
- Video Demo: [https://drive.google.com/file/d/1zoSeHsiQJi_xYmCBGOshto6U8VfHs3Gs/view?usp=sharing](Link)

